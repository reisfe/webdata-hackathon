---
title: "Cookbook for unemployment nowcast based on Google Trends"
author: "Fernando Reis, Loredana di Consiglio"
date: "4 June 2015"
output: html_document
---

This cookbook was prepared for the first Eurostat big data hackathon. The data preparation, analysis and nowcasting is performed in R.

## Step 1: a priori selection of query terms

Select which terms have a trend which could possibly help predict unemployment (subjective educated choice). This will depend on the country for which you want to produce the nowcasts.

For example for France these could be:

* 'pole emploi' is the French governmental agency which registers unemployed people, helps them find jobs and provides them with financial aid;
* 'indemnit??' refers to allocations;
* 'etre au chomage' is a query we believe unemployed people search in order to find useful resources for improving their condition.

For Italy, we could select:

* 'impiego', that is 'job';
* 'offerte lavoro', that is 'job vacancies';
* 'curriculum' is a term people looking for jobs might search in order to find useful hints and improve the chances that their resume retains employers' attention;
* 'infojobs' refers to a popular website consulted in Italy for job hunting.

In this exemple we will do it for Portugal and we will select the following search terms:

* 'desemprego';
* 'subsidio desemprego';
* 'oferta emprego'. 

## Step 2: Prepare R for the data processing

Let's prepare our workspace. Download the following GitHub repository as a zip file.

Goto https://github.com/reisfe/webdata-hackathon and click the download as a zip file button.
![][githubzip]

Unzip it to your disk. This folder will be our workspace. It includes the source file for this cookbook and in the 'scripts' sub-folder you will find a source file, 'auxiliar_functions.R', which includes some auxiliar functions we will use further ahead in this cookbook.

In order for the R code in this cookbook to work without problems you will need to set this folder as your default working directory. If the zip file were unziped to '~/webdata-hackathon' then you would run the following instruction.

```{r eval=FALSE}
setwd("~/webdata-hackathon")
```

You call `setwd` with your own folder, to where you unzipped the repository.

We will use in this cookbook three R packages, `caret`, `lubridate` and `zoo`. In order to install the packages run the following code in the R console.
```{r eval=FALSE}
install.packages(c("caret", "lubridate", "zoo"))
```

## Step 3: Get data from Google Trends (GT)

3.1. Access Google Trends website: http://www.google.com/trends/

3.2. Sign in to your Google account (if you don't have one then you will need to create it).

3.3. Enter one search term for which you want the time-series in the text box at the top.

3.4. In the section "Regional interest" click over the country you're interested, in our case "Portugal", to select only searches performed by people located in that country.

![][gt]

3.5. Select "Download as CSV" in the top right button and save the file in a sub-folder named "data" included in our workspace with a name different for each search term. For example for the search term 'desemprego' you can use the file name 'report_desemprego.csv'.

![][gtbutton]

3.6. Repeat steps 3.3 to 3.5 for the remaining search terms that you have selected in step 1.

## Step 4: Prepare the GT datasets in R

Store in variable `gtFiles` the list of data files where you saved the GT data.
```{r}
gtFiles <- c("data/report_desemprego.csv", "data/report_subsidio_desemprego.csv", "data/report_oferta_emprego.csv")
```

Open one of the csv files for inspection and note that the data starts only in line 5. Take note of the number of the last line with data. In the case of the example in this cookbook it is line number 601.
```{r}
firstLine <- 5
lastLine <- 601
```

In order to load the data into R, we use the function `read.table`. We will load all GT data files and merge them in a data frame named `gt.wk.data`.
```{r}
rm(gt.wk.data)
for (fileName in gtFiles) {
  data <- read.table(fileName, 
                     sep = ',', 
                     header = TRUE, 
                     colClasses = c("character", "integer"),
                     skip = firstLine - 1, 
                     nrows = lastLine - firstLine)
  if (exists("gt.wk.data")) {
    gt.wk.data <- cbind(gt.wk.data, data[, 2])
    } else {
      gt.wk.data <- data
      }
  }
```

Change the name of the columns of the data frame. Name the first column 'time' and the other columns according to the search terms. Then convert the first column to type date and use it to name the rows of the data frame.
```{r}
names(gt.wk.data) <- c("time", "desemprego", "subsidio desemprego", "oferta emprego")
gt.wk.data$time <- sapply(gt.wk.data$time, 
                          FUN=function(x) strsplit(x, split=" - ")[[1]][1])
gt.wk.data$time <- as.Date(gt.wk.data$time, format="%Y-%m-%d")
rownames(gt.wk.data) <- gt.wk.data$time
```

Restrict data to continous period of non-null data.
```{r}
m <- as.matrix(gt.wk.data[, 2:4])
m[m==0] <- NA
m <- na.contiguous(m)
gt.wk.data <- data.frame(m)
```

8. Convert periodicity from weekly to monthly 
We will use an auxiliar function stored in a R source file located in the scripts folder to convert the weekly GT time-series into a monthly time-series. The function is named `aggregate_week2month()` and the method it uses is to desagregate the weekly series into a daily series (using the method of Denton-Cholette) and then to aggregate the daily series to monthly.
```{r}
source("scripts/auxiliar_functions.R")
gt.mn.data.pt <- aggregate_week2month(gt.wk.data, dates=rownames(gt.wk.data))
```

Convert GT data to time-series format.
```{r}
earliest.year <- min(gt.mn.data.pt$year)
earliest.month <- min(gt.mn.data.pt[gt.mn.data.pt$year==earliest.year, ]$month)
gt.ts.pt <- ts(gt.mn.data.pt[, -(1:2)], frequency=12, start=c(earliest.year, earliest.month))
```

## Step 5: Get unemployment data from Eurostat website

Official monthly unemployment data published by Eurostat can be obtained from the database in the [website](http://ec.europa.eu/eurostat/web/main/home). The table code is 'une_nb_m' and you can use the search text box to get it.

![][3]

In the list of search results the database table should come at the top and will include several icons in the top right. Access the first one corresponding to 'Access Data Explorer'. This will take you to a new browser page with the data explorer.

![][4]

Select all the months from January 2008 to the most recent month for which data is available. For that click over the plus sign next to the 'TIME' box and in the following window select the corresponding months.
In the top right of the data explorer you have several icons. Click over the third one corresponding to 'Download'. In the new page shown click over 'Download in CSV Format'. Save the file in the sub-folder named "data" where you saved the GT data.

## Step 6: Prepare Eurostat dataset

The dataset is downloaded as a CSV file.

When downloading the dataset, even if some of the table dimensions are fixed they are still present in the data file. They can be skipped by defining the column class as 'NULL' when reading the CSV file.
```{r}
estat_file <- unz("data/une_nb_m.zip", filename="une_nb_m_1_Data.csv")
estat.mn.data <- read.csv(file=estat_file, 
                    na.strings=":", 
                    colClasses=c("character", "factor", "NULL", "NULL", "NULL", 
                                 "character", "character"))
estat.mn.data$Value <- as.numeric(gsub(",", "", estat.mn.data$Value))
```

Select the data to your country of interest, in our case "Portugal".
```{r}
estat.mn.data.pt <- estat.mn.data[estat.mn.data$GEO=="Portugal", ]
```

Time reference in variable TIME was converted to date format. (A fictitious day needs to be added to the time reference in order for it to be converted to date.)
```{r}
estat.mn.data.pt$TIME <- as.Date(paste(estat.mn.data.pt$TIME, "01", sep="D"), format="%YM%mD%d")
```

Convert the data frame to time-series format.
```{r}
earliest <- min(estat.mn.data.pt$TIME)
earliest.year <- as.numeric(format(earliest, format = "%Y"))
earliest.month <- as.numeric(format(earliest, format = "%m"))
estat.ts.pt <- ts(estat.mn.data.pt$Value, frequency=12, start=c(earliest.year, earliest.month))
```

Inspect the starting and ending periods of the data from eurostat and from GT.
```{r}
start(estat.ts.pt)
end(estat.ts.pt)
start(gt.ts.pt)
end(gt.ts.pt)
```

Merge Eurostat and Google Trends time-series.
```{r}
library(zoo)
estat <- as.zoo(estat.ts.pt)
ts.pt <- as.ts(merge(estat, as.zoo(gt.ts.pt)))
```

Restrict data to continous period of non-null data.
```{r}
ts.pt <- na.contiguous(ts.pt)
```

Let's plot the time-series.
```{r}
plot(ts.pt)
```

## Step 7: Estimate basic model without Google Trends

The model we will use to make predictions is an SARIMA model.

$$ y_{t}=\alpha.y_{t-1}+\beta.u_{t-1}+u_{t} $$

To estimate the parameters of the SARIMA model, we will use the R function `arima()`.

In order to assess the prediction accuracy of the model, an out-of-sample evaluation will be performed. The evaluation is made for one period ahead only, as the timeliness of Eurostat monthly average number of persons unemployed is t-1 and therefore nowcasts will consist of predictions for a forecasting horizon of one month.

We create time slices, or folders, 48 months long from the total length of the time-series: `r length(ts.pt)`.
The first slice starts in the first month of the time-series and ends in month 48. The second slice starts in month 2 and ends in month 49. And so forth.
```{r}
library(caret)
ts.folds <- createTimeSlices(y=ts.pt[, "estat"], initialWindow=48, horizon=1, fixedWindow = FALSE)
```

We then create a cycle where for each slice we estimate the model and store the result in variable `model`, make the forecast of one month and store the value of the forecasts in variable `forecast` and the real values in variable `realValue`.
```{r warning=FALSE}
forecast <- NULL
realValue <- NULL
for (i in 1:length(ts.folds$train)) {
   model <- arima(ts.pt[ts.folds$train[[i]]], 
                  order=c(1, 0, 0), 
                  seasonal = list(order = c(1, 0, 0)), 
                  method="ML")
  forecast <- c(forecast, predict(model)$pred[1])
  realValue <- c(realValue, ts.pt[ts.folds$test[[i]]])
}
```

We now can compare the predicted (i.e. forecasted) values with the real values, for example with a plot.
```{r}
plot(forecast ~ realValue)
```

One typical measure of the predictive error is the mean square error (MSE).
```{r}
error <- forecast - realValue
squareError <- error^2
MSE <- mean(squareError)
MSE
```

A more intuitive measure of predictive error is the percentual difference between the predicted value and the real value.
```{r}
percentageError <- forecast / realValue - 1
round(mean(abs(percentageError))*100, 3)
round(quantile(percentageError, c(0.025, 0.975))*100, 3)
```

## Step 8: Estimate model including Google Trends

The model we will use to make predictions is an SARIMAX model. The SARIMAX model has a similar specification as the SARIMA model, but also includes some other variables which should bring additional information that can improve the prediction.

$$ y_{t}=\alpha.y_{t-1}+\gamma_{0}.x_{t}+\gamma_{1}.x_{t-1}+\beta.u_{t-1}+u_{t} $$

Similirly to the SARIMA model we create time slices 48 months long.
```{r}
ts.folds <- createTimeSlices(y=ts.pt[, 2], initialWindow=48, horizon=1, fixedWindow = FALSE)
```

We then run the same cycle for each slice.
```{r warning=FALSE}
forecast <- NULL
realValue <- NULL
for (i in 1:length(ts.folds$train)) {
  model <- arima(ts.pt[ts.folds$train[[i]]], 
                 order=c(1, 0, 0), 
                 seasonal = list(order = c(1, 0, 0)), 
                 xreg = ts.pt[ts.folds$train[[i]], 2:4],
                 method="ML")
  forecast <- c(forecast, predict(model, newxreg = t(ts.pt[ts.folds$test[[i]], 2:4]))$pred[1])
  realValue <- c(realValue, ts.pt[ts.folds$test[[i]]])
}
```

Compare the predicted values with the real values with a plot.
```{r}
plot(forecast ~ realValue)
```

One typical measure of the predictive error is the mean square error (MSE).
```{r}
error <- forecast - realValue
squareError <- error^2
MSE <- mean(squareError)
MSE
```

A more intuitive measure of predictive error is the percentual difference between the predicted value and the real value.
```{r}
percentageError <- forecast / realValue - 1
round(mean(abs(percentageError))*100, 3)
round(quantile(percentageError, c(0.025, 0.975))*100, 3)
```

## Step 9: Make a nowcast for next month
Now that we have a prediction model, let's make a nowcast for next month.
We will estimate the model with all the sample we have.
```{r results='hide'}
mod.ts.sarimax <- arima(ts.pt[,"estat"], order=c(1,0,0), seasonal = c(1,0,0), xreg = ts.pt[, 2:4])
```

Let's see the estimated values of the parameters of the model.
```{r}
mod.ts.sarimax
```

The nowcasted unemployment for next month is:
```{r}
nowcast <- predict(mod.ts.sarimax, newxreg = gt.mn.data.pt[79, 3:5])$pred[1]
nowcast
```

How does it defer from the value of the previous month?
```{r}
nowcast - ts.pt[78, "estat"]
```

Let's plot our series with our nowcast.
```{r}
plot(ts.pt[,"estat"])
points(x=2015.250, y=nowcast, type="p")
```

[gt]: figure/GT.png "Google Trends"
[githubzip]: figure/github-zip.png "GitHub download as zip button"
[gtbutton]: figure/button.png "GT download button"
[3]: figure/estat_search.png "Eurostat search results"
[4]: figure/estat_data_explorer.png "Eurostat data explorer"